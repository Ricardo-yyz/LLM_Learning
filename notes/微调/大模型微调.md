# 大模型微调

### 原因：

1. 训练成本高，需要通用模型
2. Prompt Engineering的方式是一种相对来说容易上手的使用大模型的方式，但是它的缺点也非常明显。因为通常大模型的实现原理，都会对输入序列的长度有限制，Prompt Engineering 的方式会把Prompt搞得很长。越长的Prompt，大模型的推理成本越高，因为推理成本是跟Prompt长度的平方正向相关的。另外，Prompt太长会因超过限制而被截断，进而导致大模型的输出质量打折口，这也是一个非常严重的问题。对于个人使用者而言，如果是解决自己日常生活、工作中的一些问题，直接用Prompt Engineering的方式，通常问题不大。但对于对外提供服务的企业来说，要想在自己的服务中接入大模型的能力，推理成本是不得不要考虑的一个因素，微调相对来说就是一个更优的方案。
3. Prompt Engineering的效果达不到要求，企业又有比较好的自有数据，能够**通过自有数据，更好的提升大模型在特定领域的能力**。这时候微调就非常适用。
4. **要在个性化的服务中使用大模型的能力**，这时候针对每个用户的数据，训练一个轻量级的微调模型，就是一个不错的方案。
5. **数据安全的问题。**

### 微调方法：

- 全量微调FFT(Full Fine Tuning)
    
    优点：特定数据领域的表现会好很多。
    
    问题：
    
    训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的；
    
    灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。
    
- 部分参数微调PEFT(Parameter-Efficient Fine Tuning)
    
    为了避免上述问题所采用的方案。
    
    技术路线：
    
    监督式微调SFT(Supervised Fine Tuning)：使用人工标注数据，监督学习方法
    
    基于人类反馈的强化学习微调RLHF（Reinforcement Learning with Human Feedback）
    
    把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望；
    
    基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback)
    
    原理大致跟RLHF类似，但是反馈的来源是AI
    

### 流行PEFT方法

- **Prompt Tuning**
    
    Prompt Tuning的出发点，是基座模型(Foundation Model)的参数不变，为每个特定任务，训练一个少量参数的小模型，在具体执行特定任务的时候按需调用。
    
    Prompt Tuning的基本原理是在输入序列X之前，增加一些特定长度的特殊Token，以增大生成期望序列的概率。
    
    具体来说，就是将X = [x1, x2, ..., xm]变成，X = [x`1, x`2, …., x`k; x1, x2, ..., xm], Y = WX`。
    
    根据我们在《揭密Transformer：大模型背后的硬核技术》一文中介绍的大模型背后的Transformer模型，Prompt Tuning是发生在Embedding这个环节的。
    
    如果将大模型比做一个函数：Y=f(X)，那么Prompt Tuning就是在保证函数本身不变的前提下，在X前面加上了一些特定的内容，而这些内容可以影响X生成期望中Y的概率。
    
    Prompt Tuning的具体细节，可以参见：The Power of Scale for Parameter-Efficient Prompt Tuning
    
- **Prefix Tuning**
    
    Prefix Tuning的灵感来源是，基于Prompt Engineering的实践表明，在不改变大模型的前提下，在Prompt上下文中添加适当的条件，可以引导大模型有更加出色的表现。
    
    Prefix Tuning的出发点，跟Prompt Tuning的是类似的，只不过它们的具体实现上有一些差异。
    
    Prompt Tuning是在Embedding环节，往输入序列X前面加特定的Token。
    
    而Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。
    
    具体来说，就是将Y=WX中的W，变成W`=[Wp, W], Y=W`X。
    
    Prefix Tuning也保证了基座模型本身是没有变的，只是在推理的过程中，按需要在W前面拼接一些参数。
    
    Prefix Tuning的具体细节，可以参见：Prefix-Tuning: Optimizing Continuous Prompts for Generation
    
- **LoRA**
    
    LoRA背后有一个假设：我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。
    
    通俗讲人话：大模型参数很多，但并不是所有的参数都是发挥同样作用的；大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型。
    
    LoRA的基本思路，包括以下几步：
    
    首先, 要适配特定的下游任务，要训练一个特定的模型，将Y=WX变成Y=(W+∆W)X，这里面∆W主是我们要微调得到的结果；
    
    其次，将∆W进行低维分解∆W=AB (∆W为m * n维，A为m * r维，B为r * n维，r就是上述假设中的低维)；
    
    接下来，用特定的训练数据，训练出A和B即可得到∆W，在推理的过程中直接将∆W加到W上去，再没有额外的成本。
    
    另外，如果要用LoRA适配不同的场景，切换也非常方便，做简单的矩阵加法即可：(W + ∆W) - ∆W + ∆W`。
    
    关于LoRA的具体细节，可以参见LoRA: Low-Rank Adaptation of Large Language Models